---
title: "Approximating the Viterbi path of triplet Markov chains"
date: 2025-08-23
---

In this post we will introduce the problem of finding the Viterbi path of triplet Markov chains. Then we will describe one way to approximate the path using methods from variational infrence.

## Triplet Markov chains and Viterbi path

{% raw %}
Let $$\{Z_t\}_t$$ be a Markov chain with an initial probability distribution $$\pi$$ and a (homogeneous) transition matrix $$p$$. A triplet Markov chain can be described when we have $$Z_t = (U_t,X_t,Y_t)$$. Introducing the extra variables has the following goal --- we can describe a fixed data variable $$\textbf{y} \in \mathcal{Y}^T$$, do inference to find properties about $$\textbf{X} \in \mathcal{X}^T$$ that we are interested in while incorporating the effects of a nuisance variable $$\textbf{U} \in \mathcal{U}^T$$ in our model. Triplet Markov chains generalise pairwise Markov chains that have been both studied by JÃ¼ri Lember in Tartu University. It is easy to see that these models generalise Markov chains and hidden Markov models.

Let us fix the observations $$\textbf{y} = (y_t)_{t=1}^T$$. We can describe the process $$ \{U_t,X_t | \textbf{y} \}_t $$ as an discrete inhomogeneous pairwise Markov chain with a transition matrix at timestep $$t$$ noted by $$p_t$$. We can introduce the{% endraw %}  **Viterbi path problem** {% raw %} as finding the argmax $$\textbf{x}^*$$ for the following problem
\begin{equation}
\max_{\textbf{x}} \sum_{\textbf{u}} p_t(\textbf{u},\textbf{x})\tag{1}
\label{eq:test}
\end{equation}
{% endraw %}

This is known to be a NP hard problem, so we introduce a way to approximate the Viterbi path.

## Variational infrence on discrete probability distributions

As concluded in the previous chapter, we are interested in discrete probability distributions. This makes our life a bit easier than what others usually focus on [in literature](https://cse.buffalo.edu/faculty/mbeal/thesis/).

We approximate the Viterbi path by creating a variational distribution $$q$$ that approximates the true distribution $$p$$. Let us clarify which $$p$$ we are talking about. Our wish is to minimise the following Kullback-Leibler divergence on distributions over $$\mathcal{U}^T \times \mathcal{X}^T$$
{% raw %}$$ D\left[ q(\textbf{u},\textbf{x} \| p(\textbf{u},\textbf{x} | \textbf{y}) \right]. $${% endraw %}

The problems in our case are again made easy, because the distribution {% raw %}$$p(\cdot, \cdot | \textbf{y})$${% endraw %} is tractable. Otherwise it is done by introducing the evidence lower bound $$L$$ or [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound) and maximising $$L$$ instead of directly minimising the KL divergence.

It is well known that for the KL divergence the minimal $$q$$ is in fact $$p$$. But this still leaves us with the NP hard problem. So we choose to add some constraints on $$q$$ that would make finding the Viterbi path tractable. The two constraints we look at are
* $$q_{\text{BP}}(\textbf{u},\textbf{x}) = q(\textbf{u})q(\textbf{x})$$ inducing the belief propagation (BP) algorithm
* $$q_{\text{VMP}}(\textbf{u},\textbf{x}) = \prod_{t=1}^T q_t(u_t,x_t)$$ inducing the variational message passing algorithm.

**To recap** we are finding some distributions $$q_{\text{BP}}, q_{\text{VMP}}$$ that closely match the posterior (in terms of the KL divergence) and then we find the Viterbi path using these variational distributions resulting in a Viterbi path approximation.

But how would one obtain the distributions that have a minimal KL divergence under these constraints? Let us derive the optimal update rules for the VMP algorithm. This turns out to be quite similar to the [EM algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).

Let the initialisation be such that we have distributions $$q_1^{(0)},\ldots,q_T^{(0)}$$ that have a finite KL divergence. We fix some $$t$$ and find $$q^{(1)}_t$$ that minimises the KL divergence while having

$$q_{\setminus t} := q_1^{(0)} \times \ldots q_{t-1}^{(0)} \times q_{t+1}^{(0)} \times \ldots \times q_T^{(0)}$$

be fixed. This way we can describe an iterative approach where we are looking at a problem

{% raw %}$$ \min_{q_t} D\left[ q_t \times q_{\setminus t} \| p \right] $${% endraw %}

Turns out that the solution for this problem is

{% raw %}$$ q_t^{(1)}(u_t,x_t) = \frac{1}{Z_t} \sum_{ \textbf{u}_{\setminus t}, \textbf{x}_{\setminus t}} q_{\setminus t}(\textbf{u}_{\setminus t}, \textbf{x}_{\setminus t}) \ln p(\textbf{u}, \textbf{x} | \textbf{y}), $${% endraw %}

where $$Z_t$$ is the normalising constant.
